{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verification Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check IDs format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verif_format_ids(data, data_id_column):\n",
    "    # Define the expected format pattern for the IDs\n",
    "    expected_format_pattern = r'^LIDC-IDRI-\\d{4}$'\n",
    "\n",
    "    # Initialize a list to store IDs that don't match the format\n",
    "    different_format_ids = []\n",
    "\n",
    "    # Check each ID in the specified column, converting to strings as needed\n",
    "    for id_value in data[data_id_column]:\n",
    "        id_str = str(id_value)  # Convert the ID value to a string\n",
    "        # Check if the ID matches the expected format pattern using regular expressions\n",
    "        if not re.match(expected_format_pattern, id_str):\n",
    "            different_format_ids.append(id_value)  # Store IDs that don't match the format\n",
    "\n",
    "    # Output messages based on the verification results\n",
    "    if not different_format_ids:  # If no IDs with different formats were found\n",
    "        print(f\"The format of IDs in column '{data_id_column}' is the same (LIDC-IDRI- followed by a four-digit number).\")\n",
    "    else:  # If IDs with different formats were found\n",
    "        print(f\"The format of IDs in column '{data_id_column}' is not the same.\")\n",
    "        print(\"IDs with different formats:\")\n",
    "        for id_value in different_format_ids:  # Print the IDs that don't match the expected format\n",
    "            print(id_value)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check duplicated IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicated(data, data_id_column):\n",
    "    # Find duplicated IDs\n",
    "    duplicated_ids = data[data_id_column][data[data_id_column].duplicated(keep=False)]\n",
    "\n",
    "    # Print duplicate IDs (or not)\n",
    "    if not duplicated_ids.empty:\n",
    "        print(\"Duplicated IDs in column '\" + data_id_column + \"':\")\n",
    "        #diz qual o valor duplicado\n",
    "        print(duplicated_ids.unique())\n",
    "    else:\n",
    "        print(\"No duplicated IDs found in column '\" + data_id_column + \"'.\")\n",
    "        \n",
    "def duplicated(data, data_id_column):\n",
    "    # Find duplicated IDs within the specified column\n",
    "    duplicated_ids = data[data_id_column][data[data_id_column].duplicated(keep=False)]\n",
    "\n",
    "    # Print duplicate IDs (if any)\n",
    "    if not duplicated_ids.empty:  # Check if there are any duplicated IDs\n",
    "        print(\"Duplicated IDs in column '\" + data_id_column + \"':\")\n",
    "        print(duplicated_ids.unique())  # Print the unique values of the duplicated IDs\n",
    "    else:\n",
    "        print(\"No duplicated IDs found in column '\" + data_id_column + \"'.\")  # Print a message if no duplicates were found\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if doesn't exist diferent ids between two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_different_ids(data1, data2, column1, column2):\n",
    "    # Get unique IDs from both columns\n",
    "    unique_ids1 = set(data1[column1].unique())\n",
    "    unique_ids2 = set(data2[column2].unique())\n",
    "    \n",
    "    # Check for differences\n",
    "    different_ids = unique_ids1.symmetric_difference(unique_ids2)\n",
    "    \n",
    "    return len(different_ids) > 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if there are still IDs with white spaces after removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids_with_whitespace(data, data_id_column):\n",
    "    # Check for IDs with whitespace in df_metadata\n",
    "    ids_with_whitespace_data = data[data[data_id_column].str.contains(' ')]\n",
    "\n",
    "    # Print IDs with whitespace, if any\n",
    "    if not ids_with_whitespace_data.empty:\n",
    "        print(\"IDs with whitespace in :\")\n",
    "        print(ids_with_whitespace_data)\n",
    "    else:\n",
    "        print(\"No IDs with whitespace\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_data(data):\n",
    "    # Check missing values in all columns\n",
    "    missing = data.isna().sum()\n",
    "    \n",
    "    # Filter columns with missing values\n",
    "    columns_with_missing_data = missing[missing > 0]\n",
    "    \n",
    "    # Print columns with missing data and their counts\n",
    "    for column, count in columns_with_missing_data.items():\n",
    "        print(f\"Missing values in {column}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_missing_values(dataframe):\n",
    "    # Create an empty dictionary to store the results\n",
    "    missing_values_dict = {}\n",
    "    \n",
    "    # Iterate over columns\n",
    "    for column in dataframe.columns:\n",
    "        # Get rows with missing values in the current column\n",
    "        missing_rows = dataframe.index[dataframe[column].isna()].tolist()\n",
    "        \n",
    "        # If there are missing rows in the column, store them in the dictionary\n",
    "        if missing_rows:\n",
    "            missing_values_dict[column] = missing_rows\n",
    "      \n",
    "    if not missing_values_dict: # If the dictionary is empty (no missing values found)\n",
    "        print(\"No missing values found in the DataFrame.\")\n",
    "    \n",
    "    return missing_values_dict # Return the dictionary containing columns and their missing row indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_null_values(data):\n",
    "\n",
    "    null_count = data.isnull().sum().sum()  # Get the total count of null values\n",
    "    has_null_values = null_count > 0  # Check if there are null values\n",
    "\n",
    "    return has_null_values, null_count # Return a tuple with information about null values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for common IDs after processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_common_ids(dataset1, column_id_dataset1, dataset2, column_id_dataset2):\n",
    "    # Extract unique IDs from each dataset based on the provided columns\n",
    "    unique_subject_ids_metadata = dataset1[column_id_dataset1].unique()\n",
    "    unique_tcia_ids_nodule = dataset2[column_id_dataset2].unique()\n",
    "\n",
    "    # Find the common IDs between the two datasets\n",
    "    common_ids = set(unique_subject_ids_metadata) & set(unique_tcia_ids_nodule)\n",
    "\n",
    "    if len(common_ids) == 0:  # If no common IDs are found\n",
    "        print(\"No common IDs found between the two datasets.\")\n",
    "    else:  # If common IDs are found\n",
    "        print(\"Common IDs found between the two datasets:\")\n",
    "        print(common_ids)  # Display the common IDs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for differences in IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_ids(dataset1, column_id_dataset1, dataset2, column_id_dataset2):\n",
    "    # Extract IDs and convert them into sets for both datasets\n",
    "    subject_ids_metadata = set(dataset1[column_id_dataset1])\n",
    "    tcia_ids_nodule = set(dataset2[column_id_dataset2])\n",
    "\n",
    "    # Check if the sets of IDs are equal (if all IDs in both datasets match)\n",
    "    if subject_ids_metadata == tcia_ids_nodule:\n",
    "        print(\"All IDs in both datasets match.\")\n",
    "    else:\n",
    "        print(\"IDs in the datasets do not match.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for duplicated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_duplicate_columns(data):\n",
    "    duplicate_columns = set()  # Initialize an empty set to store columns with duplicate values\n",
    "    \n",
    "    # Iterate through each pair of columns and check if they have the same values\n",
    "    for i in range(len(data.columns)):\n",
    "        col1 = data.iloc[:, i]  # Get the first column for comparison\n",
    "        for j in range(i + 1, len(data.columns)):\n",
    "            col2 = data.iloc[:, j]  # Get the second column for comparison\n",
    "            if col1.equals(col2):  # Check if the two columns have the same values\n",
    "                duplicate_columns.add(data.columns[i])  # Add column names to the set if their values match\n",
    "                duplicate_columns.add(data.columns[j])\n",
    "    \n",
    "    return list(duplicate_columns)  # Convert the set to a list and return the columns with duplicate values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDs Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_ids(dataset1, column_id_dataset1, dataset2, column_id_dataset2):\n",
    "    # Extract unique IDs for each dataset based on the specified columns\n",
    "    unique_subject_ids_metadata = set(dataset1[column_id_dataset1])\n",
    "    unique_tcia_ids_nodule = set(dataset2[column_id_dataset2])\n",
    "\n",
    "    # Find IDs that exist only in one dataset and not in the other\n",
    "    ids_only_in_metadata = unique_subject_ids_metadata - unique_tcia_ids_nodule\n",
    "    ids_only_in_nodule = unique_tcia_ids_nodule - unique_subject_ids_metadata\n",
    "\n",
    "    # Print the IDs that are unique to each dataset\n",
    "    print(\"IDs only in metadata:\", ids_only_in_metadata)\n",
    "    print(\"IDs only in nodule:\", ids_only_in_nodule)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove WhiteSpaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_whiteSpace(data, data_id_column):\n",
    "    data[data_id_column] = data[data_id_column].str.strip()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove missing values\n",
    "def remove_missing_data(data, data_column):\n",
    "    data.dropna(subset=[data_column], inplace=True)\n",
    "\n",
    "# Fill missing values in the specified column with an empty string\n",
    "def fill_missing_data(data, data_id_column):\n",
    "    data[data_id_column].fillna('', inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exclude Unknown rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unknown_rows(data, data_column, value_to_remove=0):\n",
    "    # Filter the DataFrame to exclude rows where the value in data_column matches value_to_remove:\n",
    "    data = data[data[data_column] != value_to_remove]\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process duplicated data using mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_duplicated(data, data_id_column):\n",
    "    # Drop duplicate rows based on the specified column, keeping the first occurrence\n",
    "    data.drop_duplicates(subset=data_id_column, keep='first', inplace=True)\n",
    "\n",
    "    # Group the data by the specified column and calculate the mean of numeric columns\n",
    "    data = data.groupby(data_id_column).mean(numeric_only=True).reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get common ids_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_ids(dataset1, column_id_dataset1, dataset2, column_id_dataset2):\n",
    "    # Extract unique IDs from each dataset based on the provided columns\n",
    "    unique_subject_ids_metadata = dataset1[column_id_dataset1].unique()\n",
    "    unique_tcia_ids_nodule = dataset2[column_id_dataset2].unique()\n",
    "\n",
    "    # Find the common IDs between the two datasets\n",
    "    common_ids = list(set(unique_subject_ids_metadata) & set(unique_tcia_ids_nodule))\n",
    "\n",
    "    return common_ids  # Return a list of common IDs found in both datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change the ID format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_prefix(data, data_column_id, prefix):\n",
    "    # Remove the prefix\n",
    "    data[data_column_id] = data[data_column_id].str.replace(prefix, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_null_values(data):\n",
    "    # Remove rows with any null (NaN) values\n",
    "    data_cleaned = data.dropna()\n",
    "    return data_cleaned  # Return the DataFrame without null values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace Nan with Zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_nan_with_zero(df):\n",
    "    # Fill NaN values with zero and return the modified DataFrame\n",
    "    return df.fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Columns with dtype string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_string_columns(dataframe):\n",
    "    string_columns = dataframe.select_dtypes(include=['object']).columns\n",
    "    # Return the Index of columns containing string data\n",
    "    return string_columns\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
